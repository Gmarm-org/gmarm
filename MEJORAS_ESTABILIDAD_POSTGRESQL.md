# üîí Mejoras de Estabilidad para PostgreSQL

## üéØ Objetivo

Garantizar que PostgreSQL NO se caiga por problemas de memoria (OOM) ni fugas de conexiones en DEV y PROD.

## üîß Problemas Identificados

### ANTES (Problem√°tico):
```yaml
POSTGRES_MAX_CONNECTIONS: "100"         # ‚Üê DEMASIADO ALTO
POSTGRES_SHARED_BUFFERS: "512MB"        # ‚Üê Alto para 1.5GB l√≠mite
memory: 1536M                           # ‚Üê Riesgo de OOM
hikari.maximum-pool-size=15             # ‚Üê Pool alto
```

**Resultado**: PostgreSQL consum√≠a m√°s memoria de la disponible ‚Üí **OOM Killer** ‚Üí **PostgreSQL ca√≠a** ‚Üí **Connection refused**

## ‚úÖ Soluciones Implementadas

### 1. **Configuraci√≥n PostgreSQL Conservadora**

#### DEV (`docker-compose.dev.yml`):
```yaml
POSTGRES_MAX_CONNECTIONS: "30"          # Reducido de 50
POSTGRES_SHARED_BUFFERS: "128MB"        # Reducido de 256MB
POSTGRES_WORK_MEM: "4MB"                # Reducido dr√°sticamente
POSTGRES_MAINTENANCE_WORK_MEM: "32MB"   # Reducido
POSTGRES_CHECKPOINT_COMPLETION_TARGET: "0.9"  # Checkpoints suaves
POSTGRES_WAL_BUFFERS: "16MB"            # Buffers WAL peque√±os
memory: 768M                            # L√≠mite conservador
```

#### PROD (`docker-compose.prod.yml`):
```yaml
POSTGRES_MAX_CONNECTIONS: "50"          # Reducido de 100
POSTGRES_SHARED_BUFFERS: "256MB"        # Reducido de 512MB (25% de l√≠mite)
POSTGRES_WORK_MEM: "8MB"                # Reducido
POSTGRES_MAINTENANCE_WORK_MEM: "64MB"   # Reducido de 128MB
POSTGRES_CHECKPOINT_COMPLETION_TARGET: "0.9"
POSTGRES_WAL_BUFFERS: "16MB"
memory: 1024M                           # Reducido de 1536M
```

### 2. **Configuraci√≥n HikariCP Conservadora**

#### DEV (`application-docker.properties`):
```properties
spring.datasource.hikari.maximum-pool-size=8     # 26% de max_connections
spring.datasource.hikari.minimum-idle=2
spring.datasource.hikari.idle-timeout=600000     # 10min
spring.datasource.hikari.max-lifetime=1800000    # 30min
spring.datasource.hikari.leak-detection-threshold=60000
spring.datasource.hikari.validation-timeout=5000
spring.datasource.hikari.initialization-fail-timeout=1
```

#### PROD (`application-prod.properties`):
```properties
spring.datasource.hikari.maximum-pool-size=10     # 20% de max_connections
spring.datasource.hikari.minimum-idle=3
spring.datasource.hikari.idle-timeout=600000
spring.datasource.hikari.max-lifetime=1800000
spring.datasource.hikari.leak-detection-threshold=60000
spring.datasource.hikari.validation-timeout=5000
spring.datasource.hikari.register-mbeans=true     # JMX monitoring
```

### 3. **Healthchecks Mejorados**

#### DEV:
```yaml
healthcheck:
  test: ["CMD-SHELL", "pg_isready -U postgres -d gmarm_dev"]
  interval: 30s        # Aumentado de 15s
  timeout: 10s
  retries: 3           # Reducido de 5
  start_period: 90s    # Aumentado de 60s
```

#### PROD:
```yaml
healthcheck:
  test: ["CMD-SHELL", "pg_isready -U produser -d gmarm_db"]
  interval: 30s        # Aumentado de 15s
  timeout: 10s
  retries: 3           # Reducido de 5
  start_period: 90s    # Aumentado de 60s
```

### 4. **Script de Monitoreo**

Nuevo script: `scripts/monitor-postgres-health.sh`

**Caracter√≠sticas**:
- ‚úÖ Detecta conexiones activas/idle/idle in transaction
- ‚úÖ Detecta potenciales leaks de conexiones
- ‚úÖ Muestra uso de memoria del contenedor
- ‚úÖ Alerta cuando las conexiones est√°n altas

**Uso**:
```bash
./scripts/monitor-postgres-health.sh
```

## üìä Comparaci√≥n Antes/Despu√©s

| Configuraci√≥n | ANTES | DESPU√âS | Mejora |
|---------------|-------|---------|--------|
| **DEV Max Connections** | 50 | 30 | -40% |
| **DEV Shared Buffers** | 256MB | 128MB | -50% |
| **DEV Work Mem** | 16MB | 4MB | -75% |
| **DEV Memory Limit** | 768MB | 768MB | Sin cambio |
| **DEV Hikari Pool** | 10 | 8 | -20% |
| **PROD Max Connections** | 100 | 50 | -50% |
| **PROD Shared Buffers** | 512MB | 256MB | -50% |
| **PROD Work Mem** | 32MB | 8MB | -75% |
| **PROD Memory Limit** | 1536M | 1024M | -33% |
| **PROD Hikari Pool** | N/A | 10 | Nueva config |

## üõ°Ô∏è Garant√≠as

### ‚úÖ Estabilidad de Memoria
- **Shared Buffers** configurado a 25% del l√≠mite de memoria
- **Work Mem** conservador para evitar picos
- **L√≠mites de memoria** m√°s bajos para evitar OOM killers

### ‚úÖ Gesti√≥n de Conexiones
- **Pool de HikariCP** configurado a 20-26% de max_connections
- **Idle timeout** aumentado para reutilizar conexiones
- **Leak detection** activado para detectar conexiones hu√©rfanas

### ‚úÖ Monitoreo y Alertas
- **Healthchecks** m√°s robustos y r√°pidos
- **Script de monitoreo** para diagn√≥stico proactivo
- **JMX** habilitado en PROD para m√©tricas

## üöÄ Pr√≥ximos Pasos Recomendados

1. **Aplicar en DEV**: 
   ```bash
   docker-compose -f docker-compose.dev.yml down -v
   docker-compose -f docker-compose.dev.yml up -d --build
   ```

2. **Monitorear con el script**:
   ```bash
   ./scripts/monitor-postgres-health.sh
   ```

3. **Verificar logs**:
   ```bash
   docker logs gmarm-postgres-dev --tail 100
   ```

4. **Aplicar en PROD** (cuando se haga deploy):
   - Los cambios ya est√°n en `docker-compose.prod.yml`
   - Los cambios en `application-prod.properties` ya est√°n aplicados

## ‚ö†Ô∏è Notas Importantes

- **NO aumentar l√≠mites** sin justificaci√≥n t√©cnica
- **Monitorear constantemente** conexiones y memoria
- **Reportar inmediatamente** si PostgreSQL se cae
- **Usar el script de monitoreo** diariamente en PROD

## üìö Referencias

- [PostgreSQL Memory Tuning](https://www.postgresql.org/docs/current/runtime-config-resource.html)
- [HikariCP Pool Sizing](https://github.com/brettwooldridge/HikariCP/wiki/About-Pool-Sizing)
- [Docker OOM Killed Issues](https://docs.docker.com/config/containers/resource_constraints/)

---
**Fecha**: 2025-10-31  
**Autor**: Sistema GMARM  
**Versi√≥n**: 1.0

